{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Description\n",
    "This notebook is part of Pose Classification Application for children ADHD Detecting.\n",
    "\n",
    "Following contents will include\n",
    "    - Training code with Pytorch\n",
    "    - Training loss and validation loss matplot\n",
    "    - Early stopping to prevent overfitting\n",
    "    - visualizing else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (1.10.1)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (0.11.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: mediapipe in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (0.8.3)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torchvision) (1.19.3)\n",
      "Requirement already satisfied: importlib-resources in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from tqdm) (5.4.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (3.19.4)\n",
      "Requirement already satisfied: six in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: attrs in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: opencv-python in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (0.37.1)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (0.15.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from importlib-resources->tqdm) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision tqdm matplotlib mediapipe pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING pyTorch Version: 1.10.1+cu102  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('USING pyTorch Version:', torch.__version__, ' Device:', DEVICE)\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe function setting\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "## 얘는 한 프레임에 pose, face, lh, rh 가 한 행에 다 flatten 되어있는 형태로 만들어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is: (180, 1662, 29) \n",
      "y shape is: (180, 6)\n"
     ]
    }
   ],
   "source": [
    "# Setup for X, y\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['sitting', 'standing','walking','running','kicking','punching'])\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = os.path.join('../video_training') \n",
    "\n",
    "# y setting\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "# X and y setting\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(1, 30):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        \n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# ??? 왜 해야하는지 모르겠다. 연산에서 사이즈 안맞아서 이렇게 했다.\n",
    "X = X.swapaxes(1,2)\n",
    "print(\"X shape is: {} \\ny shape is: {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape: (162, 1662, 29) \ttest X shape: (18, 1662, 29)\n",
      "train y shape: (162, 6) \ttest y shape: (18, 6)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "\n",
    "## x is (180, 1662, 29), 180 = 30 folders for 6 actions.\n",
    "## last 3 folders will become test data.\n",
    "\n",
    "train_dataset_x = X[0:27]\n",
    "train_dataset_y = y[0:27]\n",
    "\n",
    "test_dataset_x = X[27:30]\n",
    "test_dataset_y = y[27:30]\n",
    "\n",
    "for i in range(2,7):\n",
    "    train_dataset_x = np.concatenate((train_dataset_x, X[(i-1)*30:i*30-3]), axis = 0)\n",
    "    train_dataset_y = np.concatenate((train_dataset_y, y[(i-1)*30:i*30-3]), axis = 0)\n",
    "   \n",
    "    test_dataset_x = np.concatenate((test_dataset_x, X[i*30-3:i*30]), axis = 0)\n",
    "    test_dataset_y = np.concatenate((test_dataset_y, y[i*30-3:i*30]), axis = 0)\n",
    "\n",
    "print(\"train X shape: {} \\ttest X shape: {}\".format(train_dataset_x.shape, test_dataset_x.shape))\n",
    "print(\"train y shape: {} \\ttest y shape: {}\".format(train_dataset_y.shape, test_dataset_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "input_size = 29\n",
    "batch_size = 27\n",
    "num_layers = 1\n",
    "hidden_size = 64\n",
    "output_size = actions.shape[0]\n",
    "\n",
    "# 아래를 왜 하는지 알아보자 추후에.\n",
    "train_dataset_x = torch.tensor(train_dataset_x, dtype=torch.float32)\n",
    "train_dataset_y = torch.tensor(train_dataset_y, dtype=torch.long)\n",
    "\n",
    "test_dataset_x = torch.tensor(test_dataset_x, dtype=torch.float32)\n",
    "test_dataset_y = torch.tensor(test_dataset_y, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_dataset_x, train_dataset_y)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_dataset_x, test_dataset_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.device = DEVICE\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size, output_size), \n",
    "                                nn.Sigmoid())\n",
    "        # LSTM = input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional\n",
    "        #self.LSTM_1 = nn.LSTM(29, 64, 1)\n",
    "        #self.fc = nn.Linear(64, actions.shape[0])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size)# 초기 hidden state 설정하기.\n",
    "            # h0.shape # torch.Size([num_layer, batch_size, hidden_size])\n",
    "        out, _ = self.rnn(x, h0) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\n",
    "        #print(\"out's shape: {}\".format(out.shape))\n",
    "        #out = out.reshape(out.shape[0], -1) # many to many 전략\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = BaseModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DL model setting and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0] loss: 7.4035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.74it/s]\n",
      " 17%|█▋        | 1/6 [00:00<00:02,  2.27it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7bcff7b67176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# 모델에 넣고,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output 가지고 loss 구하고,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8bcd350bd4ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 초기 hidden state 설정하기.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# h0.shape # torch.Size([num_layer, batch_size, hidden_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(\"out's shape: {}\".format(out.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#out = out.reshape(out.shape[0], -1) # many to many 전략\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 269\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "min_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "loss_graph = [] # 그래프 그릴 목적인 loss.\n",
    "n = len(train_dataloader)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for data in tqdm(train_dataloader):\n",
    "        #print(\"epoch: {}\".format(epoch))\n",
    "        seq, target = data # 배치 데이터.\n",
    "        #print(\"seq type: {} \\tseq shape:\")\n",
    "        #print('seq shape: {} \\ttarget shape: {}'.format(seq.shape, target.shape))\n",
    "            # seq shape: torch.Size([10, 29, 1662]) \n",
    "            # target shape: torch.Size([10, 6])\n",
    "        \n",
    "        optimizer.zero_grad() # \n",
    "        out = model(seq)   # 모델에 넣고,\n",
    "        \n",
    "        loss = criterion(out, target) # output 가지고 loss 구하고,        \n",
    "        loss.backward() # loss가 최소가 되게하는\n",
    "        \n",
    "        optimizer.step() # 가중치 업데이트 해주고,\n",
    "        running_loss += loss.item() # 한 배치의 loss 더해주고,\n",
    "        \n",
    "        n_batches +=1\n",
    "        \n",
    "    running_loss /= n_batches\n",
    "    loss_graph.append(running_loss)\n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        print('[epoch: %d] loss: %.4f'%(epoch, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(loss_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model save and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "from time import time, gmtime, localtime\n",
    "\n",
    "_time = time()\n",
    "_time = gmtime(_time)\n",
    "\n",
    "_h5_file_name = '../model/RNN_ver' + str(_time.tm_mon) + str(_time.tm_mday) + '.pt'\n",
    "_h5_file_name\n",
    "\n",
    "torch.save(model.state_dict(), _h5_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model load\n",
    "\n",
    "model = BaseModel()\n",
    "model.load_state_dict(torch.load('../model/RNN_ver928.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl (9.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from pandas) (1.19.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.1.5\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function\n",
    "\n",
    "## set the Test function\n",
    "\n",
    "def predict_oneframe(image): # for just one frame prediction\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(DEVICE)\n",
    "    '''\n",
    "    #model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    predict = 0\n",
    "    cost = 0\n",
    "    n_batches = 0\n",
    "    loss_evaluate = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #image = image.to(DEVICE)\n",
    "        \n",
    "        y_hat = model(image)\n",
    "        y_hat.argmax()\n",
    "        \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_max_count = np.zeros(6)\\n\\nfor i in range(0, 1662):\\n    _max_idx = np.argmax(res[i])\\n    #print(\"max idx : \", type(_max_idx))\\n    _max_count[_max_idx] += 1\\n\\n    \\nprint(\"result: \", _max_count)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''_max_count = np.zeros(6)\n",
    "\n",
    "for i in range(0, 1662):\n",
    "    _max_idx = np.argmax(res[i])\n",
    "    #print(\"max idx : \", type(_max_idx))\n",
    "    _max_count[_max_idx] += 1\n",
    "\n",
    "    \n",
    "print(\"result: \", _max_count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1662, 6])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_oneframe(sequence_test)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Day</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Action</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "      <td>55</td>\n",
       "      <td>sitting</td>\n",
       "      <td>0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year Month Day Hour Minute   Action Duration  Count\n",
       "0   2022     9  29   20     55  sitting        0    1.0\n",
       "1   2022     9  29   20     55  sitting        0    2.0\n",
       "2   2022     9  29   20     55  sitting        0    3.0\n",
       "3   2022     9  29   20     55  sitting        0    4.0\n",
       "4   2022     9  29   20     55  sitting        0    5.0\n",
       "..   ...   ...  ..  ...    ...      ...      ...    ...\n",
       "79  2022     9  29   20     55  sitting        0   80.0\n",
       "80  2022     9  29   20     55  sitting        0   81.0\n",
       "81  2022     9  29   20     55  sitting        0   82.0\n",
       "82  2022     9  29   20     55  sitting        0   83.0\n",
       "83  2022     9  29   20     55  sitting        0   84.0\n",
       "\n",
       "[84 rows x 8 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_time_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n"
     ]
    }
   ],
   "source": [
    "# this is for test - predict_oneframe()\n",
    "import pandas as pd\n",
    "from time import time, gmtime, localtime\n",
    "\n",
    "def _get_time():\n",
    "    _ct = gmtime(time())\n",
    "    hour = _ct.tm_hour+9\n",
    "    minute = _ct.tm_min\n",
    "    \n",
    "    return _ct, hour, minute\n",
    "\n",
    "def action_record(bh, bm, ct, action, c):\n",
    "    _t = ct\n",
    "    \n",
    "    _year = _t.tm_year\n",
    "    _month = _t.tm_mon\n",
    "    _day = _t.tm_mday\n",
    "    _hour = _t.tm_hour+9\n",
    "    _min = _t.tm_min\n",
    "    _action = actions[action]\n",
    "    _du_h = _hour - bh\n",
    "    _du_m = _min - bm\n",
    "    \n",
    "    _du = _du_h*60 + _du_m\n",
    "    _co = action_count[action]\n",
    "    \n",
    "    action_time_log.loc[c-1] = [_year, _month, _day, _hour, _min, _action, 0, _co]\n",
    "    \n",
    "    if c != 1:\n",
    "        action_time_log.loc[c-2,'Duration'] = _du\n",
    "    return _hour, _min\n",
    "\n",
    "# action count, time logging\n",
    "action_count = np.zeros(actions.shape[0])\n",
    "action_time_log = pd.DataFrame(columns=['Year', 'Month', 'Day', 'Hour', 'Minute','Action','Duration','Count'])\n",
    "\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "co = 0\n",
    "_new_h = 0\n",
    "_new_m = 0\n",
    "_bef_h = 0\n",
    "_bef_m = 0\n",
    "\n",
    "cap = cv2.VideoCapture('../input2.avi')\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        #print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-29:]\n",
    "        \n",
    "        if len(sequence) == 29:\n",
    "            sequence_test = np.expand_dims(sequence, axis=0).swapaxes(1,2)\n",
    "            \n",
    "            sequence_test = torch.tensor(sequence_test, dtype=torch.float32)\n",
    "            #print(\"30fps, np expand is: \", np.expand_dims(sequence, axis=0))\n",
    "            #print(\"shape is: {}\".format(np.expand_dims(sequence, axis=0).shape))\n",
    "                # np.expand_dims(sequence, axis=0) (1, 30, 1662)\n",
    "                # ex x shape was:                  (180, 30, 1662)\n",
    "                # train X shape:                   (162, 1662, 29)\n",
    "                # sequece_test shape:              (1, 1662, 29)\n",
    "            #print(type(sequence_test))\n",
    "                # sequence_test type is numpy.ndarray\n",
    "            res = predict_oneframe(sequence_test)[0]\n",
    "                # predict_oneframe shape:          torch.Size([1, 1662, 6])\n",
    "            \n",
    "            #print('res: {}'.format(res.shape))\n",
    "                # res: torch.Size([1662, 6])\n",
    "            #print('res: {}'.format(predict_oneframe(sequence_test).shape))\n",
    "                # predict_oneframe(sequence_test) shape: torch.Size([1, 1662, 6])\n",
    "            \n",
    "            #print('res shape: {} \\targmax: {}'.format(res.shape, np.argmax(res[1])))\n",
    "                # res shape:                       torch.Size([1662, 6]) \n",
    "                # np.argmax(res):                  8\n",
    "                # 결과 action은 res[1] 에 저장되어있다\n",
    "                \n",
    "                \n",
    "            #print(actions[np.argmax(res[1660])])\n",
    "            predictions.append(np.argmax(res[1660]))\n",
    "            \n",
    "        \n",
    "        #3. Visualize and count, logging logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res[1600]): # 최근 10개의 action이 현재 분류된 action과 같으면\n",
    "            \n",
    "                if res[1600][np.argmax(res[1600])] > 0.005:                # 그리고 현재 분류된 action의 확률이 threshold를 넘으면\n",
    "                    \n",
    "                    if len(sentence) > 0:                          # 처음이 아니면\n",
    "                        if actions[np.argmax(res[1])] != sentence[-1]:     # 이전 action과 다를 때만 logging\n",
    "                            co += 1\n",
    "                            _a = np.argmax(res[1600])\n",
    "                            \n",
    "                            action_count[_a] += 1\n",
    "                            _action = actions[_a]\n",
    "                            \n",
    "                            _bef_h = _new_h\n",
    "                            _bef_m = _new_m\n",
    "                            ct, _new_h, _new_m = _get_time()\n",
    "                            # action_record(bh, bm, ct, action, c)\n",
    "                            action_record(_bef_h, _bef_m, ct, np.argmax(res[1660]), co)\n",
    "                            print(actions[np.argmax(res[1660])])\n",
    "                            #print(action_record)\n",
    "                            sentence.append(actions[np.argmax(res[1660])])\n",
    "                            \n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res)])\n",
    "                else:\n",
    "                    print(\"under threshold, prob is: {}\".format(res[1600][np.argmax(res[1600])]))\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        #cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        #cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       #cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # csv save\n",
    "    _y = ct.tm_year\n",
    "    _m = ct.tm_mon\n",
    "    _d = ct.tm_mday\n",
    "\n",
    "    _csv_name = str(_y) + str(_m) + str(_d) +'.csv'\n",
    "\n",
    "    action_time_log.to_csv('../classify_result/' + _csv_name, header=True, index=True)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct\n",
    "_y = ct.tm_year\n",
    "_m = ct.tm_mon\n",
    "_d = ct.tm_mday\n",
    "\n",
    "_csv_name = str(_y) + str(_m) + str(_d) +'.csv'\n",
    "\n",
    "action_time_log.to_csv('../classify_result/' + _csv_name, header=True, index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Result count, time log save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Result Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_220922_venv",
   "language": "python",
   "name": "mp_220922_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
