{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Description\n",
    "This notebook is part of Pose Classification Application for children ADHD Detecting.\n",
    "\n",
    "Following contents will include\n",
    "    - Training code with Pytorch\n",
    "    - Training loss and validation loss matplot\n",
    "    - Early stopping to prevent overfitting\n",
    "    - visualizing else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (1.10.1)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (0.11.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (4.64.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (3.3.4)\n",
      "Requirement already satisfied: mediapipe in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (0.8.3)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (1.1.5)\n",
      "Requirement already satisfied: dataclasses in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from torchvision) (1.19.3)\n",
      "Requirement already satisfied: importlib-resources in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from tqdm) (5.4.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: attrs in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: opencv-python in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: protobuf>=3.11.4 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (3.19.4)\n",
      "Requirement already satisfied: six in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (0.15.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from mediapipe) (0.37.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ubuntu/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages (from importlib-resources->tqdm) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision tqdm matplotlib mediapipe pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING pyTorch Version: 1.10.1+cu102  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print('USING pyTorch Version:', torch.__version__, ' Device:', DEVICE)\n",
    "\n",
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mediapipe function setting\n",
    "\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results\n",
    "\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections\n",
    "    \n",
    "def draw_styled_landmarks(image, results):\n",
    "    # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    \n",
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh])\n",
    "\n",
    "## 얘는 한 프레임에 pose, face, lh, rh 가 한 행에 다 flatten 되어있는 형태로 만들어짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape is: (180, 1662, 29) \n",
      "y shape is: (180, 6)\n"
     ]
    }
   ],
   "source": [
    "# Setup for X, y\n",
    "\n",
    "# Actions that we try to detect\n",
    "actions = np.array(['sitting', 'standing','walking','running','kicking','punching'])\n",
    "\n",
    "# Data path\n",
    "DATA_PATH = os.path.join('../video_training') \n",
    "\n",
    "# y setting\n",
    "label_map = {label:num for num, label in enumerate(actions)}\n",
    "\n",
    "# X and y setting\n",
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in np.array(os.listdir(os.path.join(DATA_PATH, action))).astype(int):\n",
    "        window = []\n",
    "        for frame_num in range(1, 30):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(sequence), \"{}.npy\".format(frame_num)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])\n",
    "        \n",
    "X = np.array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# ??? 왜 해야하는지 모르겠다. 연산에서 사이즈 안맞아서 이렇게 했다.\n",
    "X = X.swapaxes(1,2)\n",
    "print(\"X shape is: {} \\ny shape is: {}\".format(X.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train X shape: (162, 1662, 29) \ttest X shape: (18, 1662, 29)\n",
      "train y shape: (162, 6) \ttest y shape: (18, 6)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "\n",
    "## x is (180, 1662, 29), 180 = 30 folders for 6 actions.\n",
    "## last 3 folders will become test data.\n",
    "\n",
    "train_dataset_x = X[0:27]\n",
    "train_dataset_y = y[0:27]\n",
    "\n",
    "test_dataset_x = X[27:30]\n",
    "test_dataset_y = y[27:30]\n",
    "\n",
    "for i in range(2,7):\n",
    "    train_dataset_x = np.concatenate((train_dataset_x, X[(i-1)*30:i*30-3]), axis = 0)\n",
    "    train_dataset_y = np.concatenate((train_dataset_y, y[(i-1)*30:i*30-3]), axis = 0)\n",
    "   \n",
    "    test_dataset_x = np.concatenate((test_dataset_x, X[i*30-3:i*30]), axis = 0)\n",
    "    test_dataset_y = np.concatenate((test_dataset_y, y[i*30-3:i*30]), axis = 0)\n",
    "\n",
    "print(\"train X shape: {} \\ttest X shape: {}\".format(train_dataset_x.shape, test_dataset_x.shape))\n",
    "print(\"train y shape: {} \\ttest y shape: {}\".format(train_dataset_y.shape, test_dataset_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "input_size = 29\n",
    "batch_size = 27\n",
    "num_layers = 1\n",
    "hidden_size = 64\n",
    "output_size = actions.shape[0]\n",
    "\n",
    "# 아래를 왜 하는지 알아보자 추후에.\n",
    "train_dataset_x = torch.tensor(train_dataset_x, dtype=torch.float32)\n",
    "train_dataset_y = torch.tensor(train_dataset_y, dtype=torch.long)\n",
    "\n",
    "test_dataset_x = torch.tensor(test_dataset_x, dtype=torch.float32)\n",
    "test_dataset_y = torch.tensor(test_dataset_y, dtype=torch.long)\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_dataset_x, train_dataset_y)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_dataset_x, test_dataset_y)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.device = DEVICE\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_size, output_size), \n",
    "                                nn.Sigmoid())\n",
    "        # LSTM = input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional\n",
    "        #self.LSTM_1 = nn.LSTM(29, 64, 1)\n",
    "        #self.fc = nn.Linear(64, actions.shape[0])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size()[0], self.hidden_size)# 초기 hidden state 설정하기.\n",
    "            # h0.shape # torch.Size([num_layer, batch_size, hidden_size])\n",
    "        out, _ = self.rnn(x, h0) # out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\n",
    "        #print(\"out's shape: {}\".format(out.shape))\n",
    "        #out = out.reshape(out.shape[0], -1) # many to many 전략\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model = BaseModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DL model setting and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch: 0] loss: 7.4088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  4.25it/s]\n",
      " 33%|███▎      | 2/6 [00:00<00:01,  2.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7bcff7b67176>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# output 가지고 loss 구하고,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# loss가 최소가 되게하는\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 가중치 업데이트 해주고,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "min_loss = np.Inf\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "loss_graph = [] # 그래프 그릴 목적인 loss.\n",
    "n = len(train_dataloader)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for data in tqdm(train_dataloader):\n",
    "        #print(\"epoch: {}\".format(epoch))\n",
    "        seq, target = data # 배치 데이터.\n",
    "        #print(\"seq type: {} \\tseq shape:\")\n",
    "        #print('seq shape: {} \\ttarget shape: {}'.format(seq.shape, target.shape))\n",
    "            # seq shape: torch.Size([10, 29, 1662]) \n",
    "            # target shape: torch.Size([10, 6])\n",
    "        \n",
    "        optimizer.zero_grad() # \n",
    "        out = model(seq)   # 모델에 넣고,\n",
    "        \n",
    "        loss = criterion(out, target) # output 가지고 loss 구하고,        \n",
    "        loss.backward() # loss가 최소가 되게하는\n",
    "        \n",
    "        optimizer.step() # 가중치 업데이트 해주고,\n",
    "        running_loss += loss.item() # 한 배치의 loss 더해주고,\n",
    "        \n",
    "        n_batches +=1\n",
    "        \n",
    "    running_loss /= n_batches\n",
    "    loss_graph.append(running_loss)\n",
    "    \n",
    "    if(epoch % 10 == 0):\n",
    "        print('[epoch: %d] loss: %.4f'%(epoch, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(loss_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model save and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save\n",
    "from time import time, gmtime, localtime\n",
    "\n",
    "_time = time()\n",
    "_time = gmtime(_time)\n",
    "\n",
    "_h5_file_name = '../model/RNN_ver' + str(_time.tm_mon) + str(_time.tm_mday) + '.pt'\n",
    "_h5_file_name\n",
    "\n",
    "torch.save(model.state_dict(), _h5_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model load\n",
    "\n",
    "model = BaseModel()\n",
    "model.load_state_dict(torch.load('../model/RNN_ver928.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function\n",
    "\n",
    "## set the Test function\n",
    "\n",
    "def predict_oneframe(image): # for just one frame prediction\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = model.to(DEVICE)\n",
    "    '''\n",
    "    #model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    predict = 0\n",
    "    cost = 0\n",
    "    n_batches = 0\n",
    "    loss_evaluate = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #image = image.to(DEVICE)\n",
    "        \n",
    "        y_hat = model(image)\n",
    "        y_hat.argmax()\n",
    "        \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_max_count = np.zeros(6)\\n\\nfor i in range(0, 1662):\\n    _max_idx = np.argmax(res[i])\\n    #print(\"max idx : \", type(_max_idx))\\n    _max_count[_max_idx] += 1\\n\\n    \\nprint(\"result: \", _max_count)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''_max_count = np.zeros(6)\n",
    "\n",
    "for i in range(0, 1662):\n",
    "    _max_idx = np.argmax(res[i])\n",
    "    #print(\"max idx : \", type(_max_idx))\n",
    "    _max_count[_max_idx] += 1\n",
    "\n",
    "    \n",
    "print(\"result: \", _max_count)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n",
      "sitting\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2329a7c9f2aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m#print(type(sequence_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m# sequence_test type is numpy.ndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_oneframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;31m# predict_oneframe shape:          torch.Size([1, 1662, 6])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-bff1b5f1ad94>\u001b[0m in \u001b[0;36mpredict_oneframe\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m#image = image.to(DEVICE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-8bcd350bd4ce>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mh0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# 초기 hidden state 설정하기.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# h0.shape # torch.Size([num_layer, batch_size, hidden_size])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# out: RNN의 마지막 레이어로부터 나온 output feature 를 반환한다. hn: hidden state를 반환한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m#print(\"out's shape: {}\".format(out.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m#out = out.reshape(out.shape[0], -1) # many to many 전략\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Action_Recognition/Baby-Action-Recognition/mp_env/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 269\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# this is for test - predict_oneframe()\n",
    "import pandas as pd\n",
    "from time import time, gmtime, localtime\n",
    "\n",
    "def _get_time():\n",
    "    _ct = gmtime(time())\n",
    "    hour = _ct.tm_hour+9\n",
    "    minute = _ct.tm_min\n",
    "    \n",
    "    return _ct, hour, minute\n",
    "\n",
    "def action_record(bh, bm, ct, action, c):\n",
    "    _t = ct\n",
    "    \n",
    "    _year = _t.tm_year\n",
    "    _month = _t.tm_mon\n",
    "    _day = _t.tm_mday\n",
    "    _hour = _t.tm_hour+9\n",
    "    _min = _t.tm_min\n",
    "    _action = actions[action]\n",
    "    _du_h = _hour - bh\n",
    "    _du_m = _min - bm\n",
    "    \n",
    "    _du = _du_h*60 + _du_m\n",
    "    _co = action_count[action]\n",
    "    \n",
    "    action_time_log.loc[c-1] = [_year, _month, _day, _hour, _min, _action, 0, _co]\n",
    "    \n",
    "    if c != 1:\n",
    "        action_time_log.loc[c-2,'Duration'] = _du\n",
    "    return _hour, _min\n",
    "\n",
    "# action count, time logging\n",
    "action_count = np.zeros(actions.shape[0])\n",
    "action_time_log = pd.DataFrame(columns=['Year', 'Month', 'Day', 'Hour', 'Minute','Action','Duration','Count'])\n",
    "\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.5\n",
    "co = 0\n",
    "_new_h = 0\n",
    "_new_m = 0\n",
    "_bef_h = 0\n",
    "_bef_m = 0\n",
    "\n",
    "cap = cv2.VideoCapture('../input2.avi')\n",
    "# Set mediapipe model \n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Make detections\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        #print(results)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        draw_styled_landmarks(image, results)\n",
    "        \n",
    "        # 2. Prediction logic\n",
    "        keypoints = extract_keypoints(results)\n",
    "        sequence.append(keypoints)\n",
    "        sequence = sequence[-29:]\n",
    "        \n",
    "        if len(sequence) == 29:\n",
    "            sequence_test = np.expand_dims(sequence, axis=0).swapaxes(1,2)\n",
    "            \n",
    "            sequence_test = torch.tensor(sequence_test, dtype=torch.float32)\n",
    "            #print(\"30fps, np expand is: \", np.expand_dims(sequence, axis=0))\n",
    "            #print(\"shape is: {}\".format(np.expand_dims(sequence, axis=0).shape))\n",
    "                # np.expand_dims(sequence, axis=0) (1, 30, 1662)\n",
    "                # ex x shape was:                  (180, 30, 1662)\n",
    "                # train X shape:                   (162, 1662, 29)\n",
    "                # sequece_test shape:              (1, 1662, 29)\n",
    "            #print(type(sequence_test))\n",
    "                # sequence_test type is numpy.ndarray\n",
    "            res = predict_oneframe(sequence_test)[0]\n",
    "                # predict_oneframe shape:          torch.Size([1, 1662, 6])\n",
    "            \n",
    "            #print('res: {}'.format(res.shape))\n",
    "                # res: torch.Size([1662, 6])\n",
    "            #print('res: {}'.format(predict_oneframe(sequence_test).shape))\n",
    "                # predict_oneframe(sequence_test) shape: torch.Size([1, 1662, 6])\n",
    "            \n",
    "            #print('res shape: {} \\targmax: {}'.format(res.shape, np.argmax(res[1])))\n",
    "                # res shape:                       torch.Size([1662, 6]) \n",
    "                # np.argmax(res):                  8\n",
    "                # 결과 action은 res[1] 에 저장되어있다\n",
    "                \n",
    "                \n",
    "            #print(actions[np.argmax(res[1660])])\n",
    "            predictions.append(np.argmax(res[1660]))\n",
    "            \n",
    "        \n",
    "        #3. Visualize and count, logging logic\n",
    "            if np.unique(predictions[-10:])[0]==np.argmax(res[1600]): # 최근 10개의 action이 현재 분류된 action과 같으면\n",
    "            \n",
    "                if res[1600][np.argmax(res[1600])] > 0.001:                # 그리고 현재 분류된 action의 확률이 threshold를 넘으면\n",
    "                    \n",
    "                    if len(sentence) > 0:                          # 처음이 아니면\n",
    "                        if actions[np.argmax(res[1])] != sentence[-1]:     # 이전 action과 다를 때만 logging\n",
    "                            co += 1\n",
    "                            _a = np.argmax(res[1600])\n",
    "                            \n",
    "                            action_count[_a] += 1\n",
    "                            _action = actions[_a]\n",
    "                            \n",
    "                            _bef_h = _new_h\n",
    "                            _bef_m = _new_m\n",
    "                            ct, _new_h, _new_m = _get_time()\n",
    "                            # action_record(bh, bm, ct, action, c)\n",
    "                            action_record(_bef_h, _bef_m, ct, np.argmax(res[1660]), co)\n",
    "                            print(actions[np.argmax(res[1660])])\n",
    "                            #print(action_record)\n",
    "                            sentence.append(actions[np.argmax(res[1660])])\n",
    "                            \n",
    "                    else:\n",
    "                        sentence.append(actions[np.argmax(res[1660])])\n",
    "                else:\n",
    "                    print(\"under threshold, prob is: {}\".format(res[1600][np.argmax(res[1600])]))\n",
    "            if len(sentence) > 5: \n",
    "                sentence = sentence[-5:]\n",
    "\n",
    "            # Viz probabilities\n",
    "            #image = prob_viz(res, actions, image, colors)\n",
    "            \n",
    "        #cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "        #cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                       #cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        \n",
    "        # Show to screen\n",
    "        cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "        # Break gracefully\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = gmtime(time())        \n",
    "\n",
    "# csv save\n",
    "_y = ct.tm_year\n",
    "_m = ct.tm_mon\n",
    "_d = ct.tm_mday\n",
    "\n",
    "_csv_name = str(_y) + str(_m) + str(_d) +'.csv'\n",
    "\n",
    "action_time_log.to_csv('../classify_result/' + _csv_name, header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp_220922_venv",
   "language": "python",
   "name": "mp_220922_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
